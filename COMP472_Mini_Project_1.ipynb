{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWQmx2Gf2yaq"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tiEJjNLK2D7",
    "outputId": "da9ba0ba-0d2f-4716-d482-0768b7e6efbd"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "import copy\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#warnings.filterwarnings('ignore') #comment out to see warnings\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cHtKOMJKlR1"
   },
   "source": [
    "# Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNFEBi3GM5lo"
   },
   "outputs": [],
   "source": [
    "f = gzip.open('goemotions.json.gz','rb') # Open .gz zip file\n",
    "jsonFile = f.read()\n",
    "y = json.loads(jsonFile)  # Store all contents into an array\n",
    "y = np.array(y) # Convert array to np array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGQA_0Nu22XP"
   },
   "source": [
    "# Generate Pie Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "emiam0v4RnS7",
    "outputId": "b7b67c63-c313-4c98-dd07-85f529306e12"
   },
   "outputs": [],
   "source": [
    "# Extract the 2nd and 3rd columns (emotions and sentiments)\n",
    "emotions = y[:,1] \n",
    "sentiments = y[:,2]\n",
    "\n",
    "# Count the total number of each emotion/sentiment\n",
    "eValues, eCounts = np.unique(emotions, return_counts=True)\n",
    "sValues, sCounts = np.unique(sentiments, return_counts=True)\n",
    "\n",
    "efig = plt.figure(figsize=(6, 6))\n",
    "plt.pie(eCounts, labels = eValues)\n",
    "plt.show() \n",
    "efig.savefig('emotions.pdf', dpi=efig.dpi)\n",
    "sfig = plt.figure(figsize=(6, 6))\n",
    "plt.pie(sCounts, labels = sValues)\n",
    "plt.show() \n",
    "sfig.savefig('sentiments.pdf', dpi=sfig.dpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGXGfcj6K2D_"
   },
   "source": [
    "# Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgRaA5eSK2EA",
    "outputId": "d16465de-45b9-4068-8154-7e012c75b549"
   },
   "outputs": [],
   "source": [
    "corpus = y[:,0]\n",
    "#Transform corpus to word count sparse matrix\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "#Print the number of words\n",
    "print(len(vectorizer.get_feature_names_out())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhzkM4U6nbJ8"
   },
   "source": [
    "# Process Dataset (NO STOP WORDS VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "aI9MwhYgnZee",
    "outputId": "509fa3b9-bf16-4de3-d836-979bfda16389"
   },
   "outputs": [],
   "source": [
    "#Delete following line to process the dataset with no stop words (part 2.5)\n",
    "\"\"\"\" \n",
    "corpus = y[:,0]\n",
    "#Transform corpus to word count sparse matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "x = vectorizer.fit_transform(corpus)\n",
    "#Print the number of words\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5MB-QvSK2EA"
   },
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBI936oQK2EB"
   },
   "outputs": [],
   "source": [
    "#Split the dataset and the emotions/sentiments into train and test with seed 1\n",
    "corpus_nonvector_train, corpus_nonvector_test, corpus_train, corpus_test, emotions_train, emotions_test, sentiments_train, sentiments_test = train_test_split(corpus, x, emotions, sentiments, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiKtFHWtK2EB"
   },
   "source": [
    "# Base-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrCIETJpK2EB",
    "outputId": "50787ba6-f859-4470-b119-b7df650fc1f6"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierMNBemotions = MultinomialNB()\n",
    "modelMNBemotions = classifierMNBemotions.fit(corpus_train, emotions_train)\n",
    "predictMNBemotions = modelMNBemotions.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixMNBemotions = confusion_matrix(emotions_test, predictMNBemotions)\n",
    "print(classification_report(emotions_test, predictMNBemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MNB_emotions.txt\", \"w\")\n",
    "f.write(\"Base Multinomial Naive Bayes for emotions with alpha=1 \\n\")\n",
    "f.write(np.array2string(confusionMatrixMNBemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictMNBemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0Vvl0HVK2EC",
    "outputId": "d770b510-b4a5-4687-88e8-455e63f2a0b5"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierMNBsentiments = MultinomialNB()\n",
    "modelMNBsentiments = classifierMNBsentiments.fit(corpus_train, sentiments_train)\n",
    "predictMNBsentiments = modelMNBsentiments.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixMNBsentiments = confusion_matrix(sentiments_test, predictMNBsentiments)\n",
    "print(classification_report(sentiments_test, predictMNBsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MNB_sentiments.txt\", \"w\")\n",
    "f.write(\"Base Multinomial Naive Bayes for sentiments with alpha=1 \\n\")\n",
    "f.write(np.array2string(confusionMatrixMNBsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictMNBsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5v-TnJchdo2"
   },
   "source": [
    "# Top-MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdHT7ijChdo2",
    "outputId": "ce22600d-129b-4193-dd27-14fdab0c6cfe"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierTopMNBemotions = MultinomialNB()\n",
    "TMNBE = GridSearchCV(estimator=classifierTopMNBemotions, param_grid={'alpha': [0, 0.1, 0.5, 1]}, verbose=1)\n",
    "TMNBE.fit(corpus_train, emotions_train)\n",
    "print(TMNBE.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rqybk4b0hdo3",
    "outputId": "48283446-bd0b-4ef3-aeac-091c62d52930"
   },
   "outputs": [],
   "source": [
    "#Create confusion matrix and print metrics report\n",
    "predictTopMNBemotions = TMNBE.predict(corpus_test)\n",
    "confusionMatrixTopMNBemotions = confusion_matrix(emotions_test, predictTopMNBemotions)\n",
    "print(classification_report(emotions_test, predictTopMNBemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MNB_emotions.txt\", \"w\")\n",
    "f.write(\"Top Multinomial Naive Bayes (using GridSearch) for emotions with the following parameters: \\n\")\n",
    "f.write(str(TMNBE.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixTopMNBemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictTopMNBemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCzrAb9Chdo3",
    "outputId": "5234bb9b-a64c-405c-e9ec-17ac509451b4"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierTopMNBsentiments = MultinomialNB()\n",
    "TMNBS = GridSearchCV(estimator=classifierTopMNBsentiments, param_grid={'alpha': [0, 0.1, 0.5, 1]}, verbose=1)\n",
    "TMNBS.fit(corpus_train, sentiments_train)\n",
    "print(TMNBS.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxHDkrIOhdo4",
    "outputId": "036def62-a18c-4f6b-e614-1ea8f8c82294"
   },
   "outputs": [],
   "source": [
    "#Create confusion matrix and print metrics report\n",
    "predictTopMNBsentiments = TMNBS.predict(corpus_test)\n",
    "confusionMatrixTopMNBsentiments = confusion_matrix(sentiments_test, predictTopMNBsentiments)\n",
    "print(classification_report(sentiments_test, predictTopMNBsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MNB_sentiments.txt\", \"w\")\n",
    "f.write(\"Top Multinomial Naive Bayes (using GridSearch) for sentiments with the following parameters: \\n\")\n",
    "f.write(str(TMNBS.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixTopMNBsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictTopMNBsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oobixYpLSTJ"
   },
   "source": [
    "# Base-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Um4KpCQ4Ld8W",
    "outputId": "615ab03a-9e8f-4e55-f609-3041c550b40b"
   },
   "outputs": [],
   "source": [
    "classifierDTemotions = tree.DecisionTreeClassifier() \n",
    "modelDTemotions = classifierDTemotions.fit(corpus_train, emotions_train)\n",
    "predictDTemotions = modelDTemotions.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixDTemotions = confusion_matrix(emotions_test, predictDTemotions)\n",
    "print(classification_report(emotions_test, predictDTemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_DT_emotions.txt\", \"w\")\n",
    "f.write(\"Base Decision Tree for emotions with alpha=1 \\n\")\n",
    "f.write(np.array2string(confusionMatrixDTemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictDTemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOYFG06MPuJa",
    "outputId": "9b801c39-4f12-405b-d14e-b7f10a07a7a6"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierDTsentiments = tree.DecisionTreeClassifier()\n",
    "modelDTsentiments = classifierDTsentiments.fit(corpus_train, sentiments_train)\n",
    "predictDTsentiments = modelDTsentiments.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixDTsentiments = confusion_matrix(sentiments_test, predictDTsentiments)\n",
    "print(classification_report(sentiments_test, predictDTsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_DT_sentiments.txt\", \"w\")\n",
    "f.write(\"Base Decision Tree for sentiments with alpha=1 \\n\")\n",
    "f.write(np.array2string(confusionMatrixDTsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictDTsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFpMnXLrQciw"
   },
   "source": [
    "# Top-DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bc394EeqQih2",
    "outputId": "964c6bbd-74b7-4b6f-acc5-c74d554e8924"
   },
   "outputs": [],
   "source": [
    "parametersDT = {'criterion':('entropy', 'gini'), 'max_depth':[40, 120], 'min_samples_split':[40, 80, 120]}\n",
    "classifierDTemotions = GridSearchCV(tree.DecisionTreeClassifier(), parametersDT) \n",
    "\n",
    "modelDTemotions = classifierDTemotions.fit(corpus_train, emotions_train)\n",
    "predictDTemotions = modelDTemotions.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixDTemotions = confusion_matrix(emotions_test, predictDTemotions)\n",
    "print(classification_report(emotions_test, predictDTemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_DT_emotions.txt\", \"w\")\n",
    "f.write(\"Top Decision Tree for emotions with with the following parameters: \\n\")\n",
    "f.write(str(modelDTemotions.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixDTemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictDTemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dAUR_IQLiBiQ",
    "outputId": "0dc8bc51-2d0d-4fe3-aa77-aaab6b3f4cdc"
   },
   "outputs": [],
   "source": [
    "parametersDT = {'criterion':('entropy', 'gini'), 'max_depth':[40, 120], 'min_samples_split':[40, 80, 120]}\n",
    "classifierDTsentiments = GridSearchCV(tree.DecisionTreeClassifier(), parametersDT) \n",
    "\n",
    "modelDTsentiments = classifierDTsentiments.fit(corpus_train, sentiments_train)\n",
    "predictDTsentiments = modelDTsentiments.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixDTsentiments = confusion_matrix(sentiments_test, predictDTsentiments)\n",
    "print(classification_report(sentiments_test, predictDTsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_DT_sentiments.txt\", \"w\")\n",
    "f.write(\"Top Decision Tree for sentiments with the following parameters: \\n\")\n",
    "f.write(str(modelDTsentiments.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixDTsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictDTsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa4k1WfZhdo6"
   },
   "source": [
    "# Base-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxEzbNO2hdo6"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierMLPemotions = MLPClassifier()\n",
    "modelMLPemotions = classifierMLPemotions.fit(corpus_train, emotions_train)\n",
    "predictMLPemotions = modelMLPemotions.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixMLPemotions = confusion_matrix(emotions_test, predictMLPemotions)\n",
    "print(classification_report(emotions_test, predictMLPemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MLP_emotions.txt\", \"w\")\n",
    "f.write(\"Base Multi-Layered Perceptron for emotions with default parameters \\n\")\n",
    "f.write(np.array2string(confusionMatrixMLPemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictMLPemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31w-mhQThdo6"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "classifierMLPsentiments = MLPClassifier()\n",
    "modelMLPsentiments = classifierMLPsentiments.fit(corpus_train, sentiments_train)\n",
    "predictMLPsentiments = modelMLPsentiments.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixMLPsentiments = confusion_matrix(sentiments_test, predictMLPsentiments)\n",
    "print(classification_report(sentiments_test, predictMLPsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MLP_sentiments.txt\", \"w\")\n",
    "f.write(\"Base Multi-Layered Perceptron for sentiments with default parameters \\n\")\n",
    "f.write(np.array2string(confusionMatrixMLPsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictMLPsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byU8lQyrhdo6"
   },
   "source": [
    "# Top-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDOqMDYahdo6"
   },
   "outputs": [],
   "source": [
    "parametersMLP = {\n",
    "    'activation': ('logistic', 'tanh', 'relu', 'identity'),\n",
    "    'hidden_layer_sizes': ((30, 50,), (10,10,10,)),\n",
    "    'solver': ('adam', 'sgd')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqRjXAB2vCPz"
   },
   "outputs": [],
   "source": [
    "topMLPemotions = GridSearchCV(MLPClassifier(), parametersMLP)\n",
    "topMLPemotions.fit(corpus_train, emotions_train)\n",
    "predictTopMLPemotions = topMLPemotions.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixTopMLPemotions = confusion_matrix(emotions_test, predictTopMLPemotions)\n",
    "print(classification_report(emotions_test, predictTopMLPemotions, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MLP_emotions.txt\", \"w\")\n",
    "f.write(\"Top Multi-Layered Perceptron for emotions with the following parameters: \\n\") #include hyper-parameters in string\n",
    "f.write(str(topMLPemotions.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixTopMLPemotions))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(emotions_test, predictTopMLPemotions, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tdq6OfJghdo7"
   },
   "outputs": [],
   "source": [
    "topMLPsentiments = GridSearchCV(MLPClassifier(), parametersMLP)\n",
    "topMLPsentiments.fit(corpus_train, sentiments_train)\n",
    "predictTopMLPsentiments = topMLPsentiments.predict(corpus_test)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "confusionMatrixTopMLPsentiments = confusion_matrix(sentiments_test, predictTopMLPsentiments)\n",
    "print(classification_report(sentiments_test, predictTopMLPsentiments, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MLP_sentiments.txt\", \"w\")\n",
    "f.write(\"Top Multi-Layered Perceptron for sentiments with the following parameters: \\n\")  #include hyper-parameters in string\n",
    "f.write(str(topMLPsentiments.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(confusionMatrixTopMLPsentiments))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(sentiments_test, predictTopMLPsentiments, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBhlMCXqhdo8"
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43MIdhrlhdo8"
   },
   "outputs": [],
   "source": [
    "#uncomment the desired embedding model\n",
    "\n",
    "#Load the pre-trained model\n",
    "embeddings = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "#Different models for exploration\n",
    "#embeddings = gensim.downloader.load(\"fasttext-wiki-news-subwords-300\")\n",
    "#embeddings = gensim.downloader.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t5q1Qo7hdo8"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3tTviTlhdo9"
   },
   "outputs": [],
   "source": [
    "#Tokenize every word of the training set and store it into an array\n",
    "trainTokens = [\"\"]*len(corpus_nonvector_train)\n",
    "for i in range(len(corpus_nonvector_train)):\n",
    "    trainTokens[i]=word_tokenize(corpus_nonvector_train[i])\n",
    "    \n",
    "#Tokenize every word of the test set and store it into an array\n",
    "testTokens = [\"\"]*len(corpus_nonvector_test)\n",
    "for i in range(len(corpus_nonvector_test)):\n",
    "    testTokens[i]=word_tokenize(corpus_nonvector_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDTdSRGJhdo9",
    "outputId": "b74cf7b7-46e9-41e0-82bf-9c9e34a289e3"
   },
   "outputs": [],
   "source": [
    "#Print the number of tokens overall\n",
    "traintokencount=0\n",
    "for i in range(len(trainTokens)):\n",
    "    traintokencount+=len(trainTokens[i])\n",
    "testtokencount=0\n",
    "for i in range(len(testTokens)):\n",
    "    testtokencount+=len(testTokens[i])\n",
    "print(traintokencount+testtokencount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7_2sXB-hdo9"
   },
   "source": [
    "# Embedding of each post - Hits and Misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-VWQKGKQhdo9",
    "outputId": "b76e591c-09cb-438f-f6ee-3ee700f301d5"
   },
   "outputs": [],
   "source": [
    "#Loop through the test and training set counting misses\n",
    "trainmissesCount = 0\n",
    "for i in range(len(trainTokens)):\n",
    "    for j in range(len(trainTokens[i])):\n",
    "        if(not embeddings.__contains__(trainTokens[i][j])):\n",
    "            trainmissesCount+=1\n",
    "print(\"%d hits and %d misses - %2.2f%% hit-rate for the training set\" % (traintokencount-trainmissesCount, trainmissesCount, (1-trainmissesCount/traintokencount)*100))\n",
    "\n",
    "testmissesCount=0\n",
    "for i in range(len(testTokens)):\n",
    "    for j in range(len(testTokens[i])):\n",
    "        if(not embeddings.__contains__(testTokens[i][j])):\n",
    "            testmissesCount+=1\n",
    "print(\"%d hits and %d misses - %2.2f%% hit-rate for the test set\" % (testtokencount-testmissesCount, testmissesCount, (1-testmissesCount/testtokencount)*100))\n",
    "print(\"%d hits and %d misses - %2.2f%% hit-rate overall\" % (traintokencount-trainmissesCount+testtokencount-testmissesCount, trainmissesCount+testmissesCount, (1-((trainmissesCount+testmissesCount)/(traintokencount+testtokencount)))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAM2O9Pyhdo-"
   },
   "source": [
    "# Embedding of each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG5NctFVhdo-"
   },
   "outputs": [],
   "source": [
    "#Define embedding function - loops through a 2D array and converts it to embedding\n",
    "def embed(arr):\n",
    "    embeddedArr = []    \n",
    "    for i in range(len(arr)):\n",
    "        embeddedArr.append([])\n",
    "        for j in range(len(arr[i])):\n",
    "            if(embeddings.__contains__(arr[i][j])):\n",
    "                embeddedArr[i].append(embeddings[arr[i][j]])\n",
    "    return embeddedArr\n",
    "\n",
    "#Embed the tokens of the training set\n",
    "embeddedPostsTrain = []\n",
    "embeddedPostsTrain = embed(trainTokens)\n",
    "\n",
    "#Embed the tokens of the test set\n",
    "embeddedPostsTest = []\n",
    "embeddedPostsTest = embed(testTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRtPZ76Shdo-",
    "outputId": "40271af2-858b-4c02-8bbb-aae0d3c8422d"
   },
   "outputs": [],
   "source": [
    "#Clean up the embedded vectors (Remove sentences with no words appearing in the model) as well as the corresponding entries in the y vectors\n",
    "#Copy all of the y vectors\n",
    "embedEmotionsTrain = copy.deepcopy(emotions_train)\n",
    "embedEmotionsTest = copy.deepcopy(emotions_test)\n",
    "embedSentimentsTrain = copy.deepcopy(sentiments_train)\n",
    "embedSentimentsTest = copy.deepcopy(sentiments_test)\n",
    "\n",
    "#Loop through the Training vectors, deleting the entries corresponding to empty embeds\n",
    "i=0\n",
    "while(i<len(embeddedPostsTrain)):\n",
    "    if(not embeddedPostsTrain[i]):\n",
    "        embeddedPostsTrain = np.delete(embeddedPostsTrain, i)\n",
    "        embedEmotionsTrain = np.delete(embedEmotionsTrain, i)\n",
    "        embedSentimentsTrain = np.delete(embedSentimentsTrain, i)\n",
    "    else:\n",
    "        i+=1\n",
    "\n",
    "#Repeat for the test Sets\n",
    "i=0\n",
    "while(i<len(embeddedPostsTest)):\n",
    "    if(not embeddedPostsTest[i]):\n",
    "        embeddedPostsTest = np.delete(embeddedPostsTest, i)\n",
    "        embedEmotionsTest = np.delete(embedEmotionsTest, i)\n",
    "        embedSentimentsTest = np.delete(embedSentimentsTest, i)\n",
    "    else:\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzcRzlNrhdo-"
   },
   "source": [
    "# Compute average embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdBAz4dOhdo-"
   },
   "outputs": [],
   "source": [
    "#Average the embeddings for the test set\n",
    "embeddedTrain = []\n",
    "for i in range(len(embeddedPostsTrain)):\n",
    "    embeddedTrain.append(sum(embeddedPostsTrain[i])/len(embeddedPostsTrain[i]))\n",
    "    \n",
    "#Average the embeddings for the test set\n",
    "embeddedTest = []\n",
    "for i in range(len(embeddedPostsTest)):\n",
    "    embeddedTest.append(sum(embeddedPostsTest[i])/len(embeddedPostsTest[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8K7SxACvCP4"
   },
   "source": [
    "# Base-MLP: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tj10IFKXvCP4"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "embedBaseMLPEmotions = MLPClassifier().fit(embeddedTrain, embedEmotionsTrain)\n",
    "embedBaseMLPEmotions_pred = embedBaseMLPEmotions.predict(embeddedTest)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "embedBaseMLPemotions_matrix = confusion_matrix(embedEmotionsTest, embedBaseMLPEmotions_pred)\n",
    "print(classification_report(embedEmotionsTest, embedBaseMLPEmotions_pred, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MLP_emotions-Embeddings(word2vec).txt\", \"w\")\n",
    "f.write(\"Base Multi-Layered Perceptron for emotions from the embedded Reddit posts with defaut parameters \\n\")\n",
    "f.write(np.array2string(embedBaseMLPemotions_matrix))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(embedEmotionsTest, embedBaseMLPEmotions_pred, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8q-hqhFvCP5"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "embedBaseMLPSentiments = MLPClassifier().fit(embeddedTrain, embedSentimentsTrain)\n",
    "embedBaseMLPSentiments_pred = embedBaseMLPSentiments.predict(embeddedTest)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "embedBaseMLPSentiments_matrix = confusion_matrix(embedSentimentsTest, embedBaseMLPSentiments_pred)\n",
    "print(classification_report(embedSentimentsTest, embedBaseMLPSentiments_pred, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Base_MLP_sentiments-Embeddings(word2vec).txt\", \"w\")\n",
    "f.write(\"Base Multi-Layered Perceptron for sentiments from the embedded Reddit posts with default parameters \\n\")\n",
    "f.write(np.array2string(embedBaseMLPSentiments_matrix))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(embedSentimentsTest, embedBaseMLPSentiments_pred, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4B9NuxjvCP5"
   },
   "source": [
    "# Top-MLP: Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjw7MLhYvCP6"
   },
   "outputs": [],
   "source": [
    "parametersMLP = {\n",
    "    'activation': ('logistic', 'tanh', 'relu', 'identity'),\n",
    "    'hidden_layer_sizes': ((30, 50,), (10,10,10,)),\n",
    "    'solver': ('adam', 'sgd')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5EDViQHvCP6",
    "outputId": "4687310c-7e9b-4b28-cbe4-f374e0f560b1"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "embedTopMLPEmotions = GridSearchCV(MLPClassifier(), parametersMLP, cv=3, scoring='accuracy').fit(embeddedTrain, embedEmotionsTrain)\n",
    "embedTopMLPEmotions_pred = embedTopMLPEmotions.predict(embeddedTest)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "embedTopMLPEmotions_matrix = confusion_matrix(embedEmotionsTest, embedTopMLPEmotions_pred)\n",
    "print(classification_report(embedEmotionsTest, embedTopMLPEmotions_pred, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MLP_emotions-Embeddings(word2vec).txt\", \"w\")\n",
    "f.write(\"Top Multi-Layered Perceptron for emotions from the embedded Reddit posts with the following parameters: \\n\")\n",
    "f.write(str(embedTopMLPEmotions.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(embedTopMLPEmotions_matrix))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(embedEmotionsTest, embedTopMLPEmotions_pred, zero_division=0))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60pZSD2EvCP6",
    "outputId": "1f7903aa-cf9f-467c-e723-260fb56388dc"
   },
   "outputs": [],
   "source": [
    "#Create and train model\n",
    "embedTopMLPSentiments = GridSearchCV(MLPClassifier(), parametersMLP, cv=3, scoring='accuracy').fit(embeddedTrain, embedSentimentsTrain)\n",
    "embedTopMLPSentiments_pred = embedTopMLPSentiments.predict(embeddedTest)\n",
    "\n",
    "#Create confusion matrix and print metrics report\n",
    "embedTopMLPSentiments_matrix = confusion_matrix(embedSentimentsTest, embedTopMLPSentiments_pred)\n",
    "print(classification_report(embedSentimentsTest, embedTopMLPSentiments_pred, zero_division=0))\n",
    "\n",
    "#Write everything to file\n",
    "f = open(\"Top_MLP_sentiments-Embeddings(word2vec).txt\", \"w\")\n",
    "f.write(\"Top Multi-Layered Perceptron for sentiments from the embedded Reddit posts with the following parameters: \\n\")\n",
    "f.write(str(embedTopMLPSentiments.best_params_))\n",
    "f.write(\"\\n\")\n",
    "f.write(np.array2string(embedTopMLPSentiments_matrix))\n",
    "f.write(\"\\n\")\n",
    "f.write(classification_report(embedSentimentsTest, embedTopMLPSentiments_pred, zero_division=0))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sFpMnXLrQciw"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
